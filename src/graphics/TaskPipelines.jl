module TaskPipelines

import Distributed: pmap

import Vulkan as vk
import DataStructures as ds

import hardware as hw
import framework as fw
import resources as rd
import commands
import render
import Glfw as window
import pipeline as pipe

# Think of a Pipeline as gpu pipeline: one or more shaders plus all the crap to
# configure and coordinate.
#
# A PipelineExecutor, is then a logical "thread" which feeds that pipeline. Most
# of the resources which are required to create work for a queue are externally
# synchronised, which means that if you want to feed one Pipeline (or Queue)
# from more than one thread, you need multiple copies of these resources wrapped
# up in such a way that you know each copy will be used on a single thread at
# any given time. Those copies are the Executors.
#
# So we have one pipeline and one queue, with possibly many executors binding
# memory and recording command buffers in between.
abstract type Pipeline end
abstract type PipelineExecutor end

# REVIEW: Really a pipeline IO wrapper. All of the real logic is hidden in the
# closure of the thread with which these channels communicate.
#
# That isn't ideal.
struct AsyncPipeline <: Pipeline
  input::Channel
  sigkill::Channel
end

################################################################################
##### vk Queue wrappers
################################################################################

function submit(queue::vk.Queue, submissions)
  out = Channel(1)
  put!(out, vk.queue_submit_2(queue, submissions))
  out
end

struct SharedQueueInfo
  info::vk.DeviceQueueInfo2
end

struct SharedQueue
  ch
  sigkill
end

function teardown(p::SharedQueue)
  put!(p.sigkill, true)
end

function sharedqueue(queue::vk.Queue)
  ch = Channel()
  kill = Channel()
  hw.thread() do
    while !isready(kill)
      (submissions, out) = take!(ch)
      res = vk.queue_submit_2(queue, submissions)
      put!(out, res)
    end
  end

  SharedQueue(ch, kill)
end

function submit(queue::SharedQueue, submissions)
  out = Channel(1)
  put!(queue.ch, (submissions, out))
  return out
end

function getqueue(system, info::vk.DeviceQueueInfo2)
  if ds.containsp(system.cache[].queues, info)
    q = get(system.cache[].queues, info)
    @warn "Creating multiple handles to externally synchronised queue: " * string(info)
  else
    q = vk.get_device_queue_2(system.device, info)
    ds.swap!(system.cache, ds.associn, [:queues, info], q)
  end
  return q
end

function getqueue(system, info::SharedQueueInfo)
  if ds.containsp(system.cache[].queues, info)
    return get(system.cache[].queues, info)
  else
    q = sharedqueue(getqueue(system, info.info))
    ds.swap!(system.cache, ds.associn, [:queues, info], q)
    return q
  end
end

################################################################################
##### Host <-> Device Data Transfer
################################################################################

struct TransferPipeline <: Pipeline
  taskqueue
  kill
  spec
  executors
end

struct CommandPoolExecutor <: PipelineExecutor
  work
  kill
  pool
  queue
end

function cpe(system, qf, queue)
  dev = system.device

  buffercount = ds.Atom(0)
  pool = vk.unwrap(vk.create_command_pool(dev, qf))

  work = Channel()
  sigkill = Channel(1)

  hw.thread() do
    while !isready(sigkill)
      recorder = take!(work)

      if buffercount[] === 0
        @info "resetting transfer pool"
        # FIXME: pools never get reset until the pipeline is idle.
        #
        # This will probably lead to memory leaks in heavily used pipelines.
        vk.reset_command_pool(dev, pool)
      end

      ds.swap!(buffercount, +, 1)
      postsig = hw.ssi(dev)
      cmd = hw.commandbuffer(dev, pool)

      recorder(cmd, postsig, queue)

      hw.thread() do
        # REVIEW: These probably block threads. Assess that.
        commands.wait_semaphore(dev, postsig)
        ds.swap!(buffercount, -, 1)
      end
    end
  end

  CommandPoolExecutor(work, sigkill, pool, queue)
end

function transferpipeline(system, name, spec)
  dev = system.device

  qf = get(system.spec.queues.queue_families, spec.type)
  queue = getqueue(system, get(system.spec.queues.allocations, name))

  tasks = Channel(32)
  sigkill = Channel(1)
  exec = ds.vector(cpe(system, qf, queue))

  # FIXME: This indirection is stupid overengineering as presently used.
  hw.thread() do
    while !isready(sigkill)
      cb = take!(tasks)
      @info "scheduling transfer task."
      put!(exec[1].work, cb)
    end
  end

  TransferPipeline(tasks, sigkill, spec, ds.Atom(exec))
end

function register(p::TransferPipeline, cb)
  put!(p.taskqueue, cb)
end

function record(cb, p::TransferPipeline, wait=[], signal=[])
  out = Channel(1)

  function recorder(cmd, post, queue)
    vk.begin_command_buffer(cmd, vk.CommandBufferBeginInfo())

    cb(cmd)

    vk.end_command_buffer(cmd)

    cbi = vk.CommandBufferSubmitInfo(cmd, 0)

    res = submit(queue, [vk.SubmitInfo2(wait, [cbi], vcat(signal, [post]))])

    put!(out, (post, take!(res)))
  end

  register(p, recorder)

  return out

end

################################################################################
##### Compute Pipelines
################################################################################


abstract type PipelineAllocator end

struct LeakyAllocator <: PipelineAllocator
  system
  dev
  config
  layout
  dsetpool
  commandpool
  dsets
  cmdsbuffs
  outputs
  semaphores
end

function teardown(p::AsyncPipeline)
  @async begin
    try
      put!(p.sigkill, true)
    catch e
      ds.handleerror(e)
    end
  end
end

function passresources(a::LeakyAllocator)
  sem = vk.unwrap(vk.create_semaphore(
    a.dev,
    vk.SemaphoreCreateInfo(
      next=vk.SemaphoreTypeCreateInfo(vk.SEMAPHORE_TYPE_TIMELINE, UInt(1))
    )))

  post = vk.SemaphoreSubmitInfo(sem, UInt(2), 0)

  dset = vk.unwrap(vk.allocate_descriptor_sets(
    a.dev,
    vk.DescriptorSetAllocateInfo(a.dsetpool, [a.layout])
  ))[1]

  cmd = hw.commandbuffers(a.dev, a.commandpool, 1)[1]

  outputs = ds.into!(
    [],
    map(x -> allocout(a.system, x)),
    a.config.outputs
  )

  return (dset, cmd, outputs, post)
end

function dummyallocator(system, config, qf, layoutci, layout)
  dev = get(system, :device)

  dsetpool = vk.unwrap(vk.create_descriptor_pool(
    dev,
    rd.descriptorpool(layoutci, 10000)
  ))

  commandpool = hw.commandpool(dev, qf)

  LeakyAllocator(system, dev, config, layout, dsetpool, commandpool, [], [], [], [])

end

function allocout(system, config)
  type = get(config, :type)
  if type === :ssbo
    out = hw.buffer(system, config)
  elseif type === :image
    out = hw.createimage(system, config)
  else
    throw("not implemented")
  end

  ds.assoc(out, :config, config)
end

function computepipeline(system, name, config)
  dev = system.device
  qf = system.queues.compute
  # FIXME:
  queue = hw.getqueue(system, :compute)

  stage = ds.hashmap(:stage, :compute)
  stagesetter(sets) = map(set -> merge(stage, set), sets)

  if ds.containsp(config, :pushconstants)
    config = ds.update(config, :pushconstants, merge, stage)
  end

  bindings = stagesetter(vcat(get(config, :inputs, []), config.outputs))

  layoutci = rd. descriptorsetlayout(bindings)
  layout = vk.unwrap(vk.create_descriptor_set_layout(dev, layoutci))

  pipeline = pipe.computepipeline(
    dev, config.shader, layout, [config.pushconstants]
  )

  allocator = dummyallocator(system, config, qf, layoutci, layout)

  ## Coordination

  inch = Channel()
  killch = Channel()

  Threads.@spawn begin
    try
      while !isready(killch)
        (inputs, pcs, outch) = take!(inch)

        (dset, cmdbuff, outputs, postsem) = passresources(allocator)

        fw.binddescriptors(
          dev, bindings, dset, vcat(inputs, outputs)
        )

        commands.recordcomputation(
          cmdbuff,
          pipeline.pipeline,
          pipeline.layout,
          config.workgroups,
          [dset],
          ds.assoc(config.pushconstants, :value, pcs)
        )

        wait = ds.into!([], map(x -> x.wait) âˆ˜ ds.cat(), inputs)

        cmdsub = vk.CommandBufferSubmitInfo(cmdbuff, 0)

        submission = vk.SubmitInfo2(wait, [cmdsub], [postsem])

        submit(queue, [submission])

        wrap = ds.into!([], map(x -> ds.assoc(x, :wait, [postsem])), outputs)

        put!(outch, wrap)
      end

      # TODO: Cleanup:
      # No more work will be submitted on this queue, but there might be
      # submitted work waiting on something else.
      #

      # "finished" (typemax(UInt32) maybe)? So long as every semaphore in
      # every pipeline runner gets bumped, that should be enough for everything
      # to unblock and flush.
      #
      # What about presentation? That's a little different. Do we actually
      # have to present all buffered frames before exiting? Do we care?

    catch e
      print(stderr, "\n Error in pipeline thread: " *
                    string(get(config, :name)) * "\n")
      ds.handleerror(e)
    end
  end

  return AsyncPipeline(inch, killch)
end

function run(p::AsyncPipeline, inputs, pcs=[])
  # Create a fresh output channel for each set of inputs. I'm not convinced this
  # is the right way to do it, but it lets multiple people submit work and only
  # be able to see their own results.
  out = Channel()
  put!(p.input, [inputs, pcs, out])
  return out
end

function presentationpipeline(system)
end

function graphicspipeline(system, name, config)
  dev = system.device
  win = system.window

  qf = system.spec.queues.queue_families.graphics
  queue = getqueue(system, get(system.spec.queues.allocations, name))

  bindings = []
  layoutci = rd. descriptorsetlayout(bindings)
  layout = vk.unwrap(vk.create_descriptor_set_layout(dev, layoutci))

  commandpool = hw.commandpool(dev, qf)

  # Steps to initialise graphics.

  swch = hw.thread() do
    swapchain = hw.createswapchain(system, system.spec)
    iviews = hw.createimageviews(merge(system, swapchain), config)
    return merge(swapchain, iviews)
  end

  system = merge(system, pipe.renderpass(system, config))

  gpch = hw.thread() do
    pipe.creategraphicspipeline(system, ds.hashmap(name, config))
  end

  system = merge(system, take!(swch))

  system = merge(system, pipe.createframebuffers(system))

  system = merge(system, take!(gpch))

  renderstate = fw.assemblerender(system, config)

  inch = Channel()
  killch = Channel()

  Threads.@spawn begin
    framecounter = 0
    t = time()
    try
      while !isready(killch)
        framecounter += 1
        (vbuff, pcs, outch) = take!(inch)

        if window.closep(win)
          put!(outch, :closed)
          break
        end

        if window.minimised(win)
          put!(outch, :skip)
        else
          cmd = hw.commandbuffers(dev, commandpool, 1)[1]
          co = ds.assoc(render.syncsetup(system), :commandbuffer, cmd)

          if vbuff != []
            renderstate = ds.assoc(renderstate, :vertexbuffer, vbuff)
          end

          gsig = render.draw(system, co, renderstate)

          @async begin
            commands.wait_semaphore(dev, gsig)
            # Don't let GC get the command buffer prematurely.
            co
          end

          put!(outch, gsig)
        end
      end
      @info "Average fps: " * string(round(framecounter / (time() - t)))
    catch e
      print(stderr, "\n Error in pipeline thread: " * string(name) * "\n")
      ds.handleerror(e)
    end
  end

  return AsyncPipeline(inch, killch)
end

function initpipeline(system, pipeline)
  name = ds.key(pipeline)
  config = ds.val(pipeline)
  t = config.type
  if t === :transfer
    transferpipeline(system, name, config)
  elseif t === :compute
    computepipeline(system, name, config)
  elseif t === :graphics
    graphicspipeline(system, name, config)
  else
    throw("unknown pipeline type: " * string(pipeline))
  end
end

function buildpipelines(system, config)
  # TODO: Here's where we set up the pipeline cache

  map(e -> [ds.key(e), initpipeline(system, e)], config.pipelines)
  # ds.into(ds.emptymap, pmap(
  #   e -> [ds.key(e), initpipeline(system, e)],
  #   config.pipelines
  # ))
end

end
