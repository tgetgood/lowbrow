* Optimisation log
  Initial performance was unsuitable for anything but a toy. This (poorly)
  tracks my attempts to make it practical.
** notes [2023-09-18 Mon]
   For normal use these datastructures seem to perform adequately.

   However, when trying to load textures with the following code:

   #+BEGIN_SRC julia
     @time rgb = ds.into(
       ds.emptyvector,
       map(p -> [
         reinterpret(UInt8, p.b), reinterpret(UInt8, p.g), reinterpret(UInt8, p.r),
         0xff
       ])
       âˆ˜
       ds.cat(),
       ds.take(x, image)
     )
   #+END_SRC

   Where =image= is an array of pixels, I get some fun results. For x = 2-2^15:

   2:   0.031788 seconds (46.67 k allocations: 9.054 MiB, 98.30% compilation time)
   4:   0.006815 seconds (84 allocations: 12.007 MiB, 85.72% gc time)
   8:   0.001803 seconds (164 allocations: 24.023 MiB)
   16:   0.021166 seconds (3.82 k allocations: 48.276 MiB, 23.66% gc time, 40.25% compilation time)
   32:   0.016689 seconds (866 allocations: 96.120 MiB, 41.37% gc time)
   64:   0.065319 seconds (48.74 k allocations: 195.433 MiB, 15.55% gc time, 58.27% compilation time)
   128:   0.044260 seconds (4.05 k allocations: 384.511 MiB, 38.96% gc time)
   256:   0.075904 seconds (8.30 k allocations: 769.056 MiB, 31.04% gc time)
   512:   0.143548 seconds (18.83 k allocations: 1.502 GiB, 32.47% gc time)
   1024:   0.288988 seconds (39.91 k allocations: 3.005 GiB, 32.32% gc time)
   2048:   0.570507 seconds (84.10 k allocations: 6.012 GiB, 32.30% gc time)
   4096:   1.138775 seconds (172.48 k allocations: 12.030 GiB, 32.34% gc time)
   8192:   2.278270 seconds (349.25 k allocations: 24.086 GiB, 32.43% gc time)
   16384:   4.563693 seconds (702.79 k allocations: 48.270 GiB, 32.32% gc time)
   32768:   9.203719 seconds (1.48 M allocations: 96.883 GiB, 32.35% gc time)

   Which extrapolated up to the full 1M pixels of the image in question, this would
   require 20M allocations and 3TB of allocated memory. Of course my machine runs
   out of ram long before that can be tested.

   The naive jl code:

   #+BEGIN_SRC julia
     rgb::Vector{UInt8}  = reduce(
       vcat,
       map(p -> [
         reinterpret(UInt8, p.b), reinterpret(UInt8, p.g), reinterpret(UInt8, p.r),
         0xff
       ],
           image
           )
     )
   #+END_SRC

   0.577784 seconds (2.15 M allocations: 2.243 GiB, 25.86% gc time, 5.11% compilation

   So the number of allocations in the persistent case is is ~10x higher and the
   memory allocated and collected is >1000x higher.

   10x on both counts would actually be acceptable for my exploratory use case, but
   the 1000-1500x doesn't fly.

   Curiously, if I use Base.Vector, it takes longer and uses more ram. It also seg
   faults at 2^15 which I think is actually a stack overflow.

   The idea was to use Base.Vector and Base.reduce and have better performace. That
   needs a redesign.
** improvements [2023-09-28 Thu]

   After reimplementing vectors using Tuples instead of arrays (not to mention
   fixing up some blatant errors, we get: 2:   0.037318 seconds (34.10 k allocations: 2.270 MiB) 4:   0.000030 seconds (147 allocations: 3.234 KiB)
   8:   0.000036 seconds (299 allocations: 6.750 KiB)
   16:   0.012085 seconds (4.31 k allocations: 261.349 KiB)
   32:   0.021579 seconds (7.78 k allocations: 456.428 KiB)
   64:   0.040643 seconds (14.73 k allocations: 852.820 KiB)
   128:   0.079438 seconds (28.61 k allocations: 1.631 MiB)
   256:   0.154938 seconds (56.38 k allocations: 3.331 MiB)
   512:   0.005223 seconds (42.55 k allocations: 1.547 MiB)
   1024:   0.014135 seconds (102.44 k allocations: 4.144 MiB)
   2048:   0.049300 seconds (245.93 k allocations: 13.219 MiB, 25.63% gc time)
   4096:   0.088340 seconds (565.65 k allocations: 33.909 MiB)
   8192:   0.250066 seconds (1.35 M allocations: 76.654 MiB, 5.44% gc time)
   16384:   0.580798 seconds (3.10 M allocations: 166.967 MiB, 4.01% gc time)
   32768:   1.288246 seconds (6.79 M allocations: 368.141 MiB, 3.48% gc time)
   65536:   2.873495 seconds (14.92 M allocations: 894.722 MiB, 3.85% gc time)
   131072:   6.365209 seconds (32.24 M allocations: 1.982 GiB, 4.17% gc time)
   262144:  14.536195 seconds (71.52 M allocations: 4.240 GiB, 3.77% gc time)
   524288:  32.252954 seconds (155.95 M allocations: 8.910 GiB, 3.66% gc time)
   1048576:  68.632605 seconds (330.91 M allocations: 18.891 GiB, 3.62% gc time)

   ~10x improvement in run time. Huge improvement in allocated space, but number of
   allocations is ~6x higher.

   Lesson 1) transients are indispensible. Lesson 2) maybe tuples weren't such a
   hot idea after all.

   I'll try out the streamlined design with vectors again. I might be thrashing
   here.

   Easy wins are parallel transduction and transients. Maybe I should focus my
   efforts there.

   I also massively simplified the implementation of maps, and added some tests to
   convince myself they work (to the extent I've thought to use them). But again
   huge performance regression. They're basically unusable as is.

   After reimplementing with (base) vectors instead of tuples, I get:
   2:   0.000036 seconds (68 allocations: 2.328 KiB)
   4:   0.000011 seconds (138 allocations: 4.891 KiB)
   8:   0.000017 seconds (278 allocations: 10.938 KiB)
   16:   0.000050 seconds (930 allocations: 37.641 KiB)
   32:   0.000094 seconds (2.35 k allocations: 95.234 KiB)
   64:   0.000207 seconds (5.57 k allocations: 225.328 KiB)
   128:   0.000493 seconds (13.54 k allocations: 619.172 KiB)
   256:   0.001366 seconds (36.66 k allocations: 1.819 MiB)
   512:   0.003244 seconds (88.02 k allocations: 4.183 MiB)
   1024:   0.015228 seconds (193.80 k allocations: 9.068 MiB, 59.99% gc time)
   2048:   0.012516 seconds (417.67 k allocations: 19.494 MiB)
   4096:   0.036217 seconds (914.55 k allocations: 45.283 MiB, 24.31% gc time)
   8192:   0.079682 seconds (2.10 M allocations: 113.657 MiB, 18.27% gc time)
   16384:   0.158852 seconds (4.74 M allocations: 245.702 MiB, 13.64% gc time)
   32768:   0.338404 seconds (10.12 M allocations: 514.794 MiB, 14.86% gc time)
   65536:   0.701287 seconds (21.26 M allocations: 1.049 GiB, 14.67% gc time)
   131072:   1.523102 seconds (45.11 M allocations: 2.295 GiB, 16.00% gc time)
   262144:   3.396177 seconds (99.11 M allocations: 5.313 GiB, 16.90% gc time)
   524288:   7.134940 seconds (215.47 M allocations: 11.204 GiB, 17.91% gc time)
   1048576:  14.954999 seconds (451.32 M allocations: 23.141 GiB, 19.62% gc time)

   With no runtime compiler warmup to speak of, which is an added bonus.

   so now we're down to 10x ram, 30x runtime, and 200x allocations. But that's
   without transients. Progress.

   And to think I went with tuples because I thought they would optimise better
   being immutable. And they do in terms of allocations and memory usage. But 1.28x
   more RAM for a 4.5x runtime boost seems like a good trade.

   [2023-09-29 Fri]

   2:   0.000040 seconds (72 allocations: 2.141 KiB)
   4:   0.000011 seconds (142 allocations: 4.453 KiB)
   8:   0.000018 seconds (282 allocations: 10.000 KiB)
   16:   0.000050 seconds (866 allocations: 32.734 KiB)
   32:   0.000095 seconds (2.14 k allocations: 81.078 KiB)
   64:   0.000218 seconds (5.07 k allocations: 189.672 KiB)
   128:   0.000470 seconds (12.47 k allocations: 528.500 KiB)
   256:   0.001161 seconds (33.42 k allocations: 1.561 MiB)
   512:   0.002527 seconds (77.35 k allocations: 3.539 MiB)
   1024:   0.005505 seconds (168.29 k allocations: 7.620 MiB)
   2048:   0.011057 seconds (362.45 k allocations: 16.345 MiB)
   4096:   0.032831 seconds (799.94 k allocations: 38.357 MiB, 28.70% gc time)
   8192:   0.071598 seconds (1.87 M allocations: 97.677 MiB, 23.86% gc time)
   16384:   0.130356 seconds (4.11 M allocations: 209.177 MiB, 12.99% gc time)
   32768:   0.281846 seconds (8.69 M allocations: 436.178 MiB, 16.36% gc time)
   65536:   0.582891 seconds (18.24 M allocations: 908.179 MiB, 16.01% gc time)
   131072:   1.223732 seconds (38.90 M allocations: 1.951 GiB, 16.16% gc time)
   262144:   2.918031 seconds (86.53 M allocations: 4.558 GiB, 17.62% gc time)
   524288:   6.131520 seconds (184.93 M allocations: 9.550 GiB, 18.30% gc time)
   1048576:  12.946524 seconds (384.87 M allocations: 19.658 GiB, 20.43% gc time)

   Just by overloading conj with a fast case when the element being added is a
   subtype of the collection type.

   I didn't expect reflection (typejoin) to be cheap, but that one call can't
   account for what's going on here. Looking at the LLVM bitcode, the generate code
   is superficially similar, but I'm not familiar enough to compare them without
   great effort which I'd rather allocate elsewhere.

   Maybe allowing the possibility of having to box at every step causes all the
   extra work. In any case I've learned to be much more wary of reflection.

   And removing the asserts gets us:

   2:   0.266746 seconds (696.97 k allocations: 46.322 MiB, 99.87% compilation time)
   4:   0.000036 seconds (142 allocations: 4.453 KiB)
   8:   0.000034 seconds (282 allocations: 10.000 KiB)
   16:   0.188647 seconds (130.75 k allocations: 8.612 MiB, 8.30% gc time, 99.93% compilation time)
   32:   0.000098 seconds (1.85 k allocations: 72.125 KiB)
   64:   0.000166 seconds (3.95 k allocations: 161.656 KiB)
   128:   0.000365 seconds (8.14 k allocations: 438.359 KiB)
   256:   0.000868 seconds (16.52 k allocations: 1.250 MiB)
   512:   0.008639 seconds (42.78 k allocations: 2.864 MiB, 77.68% compilation time)
   1024:   0.003965 seconds (94.47 k allocations: 6.089 MiB)
   2048:   0.008332 seconds (198.40 k allocations: 12.750 MiB)
   4096:   0.033216 seconds (406.25 k allocations: 29.133 MiB, 47.31% gc time)
   8192:   0.049455 seconds (821.97 k allocations: 70.945 MiB, 25.66% gc time)
   16384:   0.100161 seconds (1.98 M allocations: 153.680 MiB, 16.17% gc time)
   32768:   0.188255 seconds (4.29 M allocations: 320.151 MiB, 10.64% gc time)
   65536:   0.394972 seconds (8.92 M allocations: 659.092 MiB, 12.61% gc time)
   131072:   0.855680 seconds (18.18 M allocations: 1.401 GiB, 13.63% gc time)
   262144:   1.972930 seconds (36.69 M allocations: 3.199 GiB, 15.42% gc time)
   524288:   4.259396 seconds (84.20 M allocations: 6.769 GiB, 17.15% gc time)
   1048576:   8.968445 seconds (179.22 M allocations: 13.939 GiB, 18.54% gc time)

   Which really oughtn't be surprising...

   One idea which I ought to look into is preallocating vectorleaves in certain
   contexts. Creating vectors with Vector(undef, 32) and tracking indicies instead
   of using `end` actually caused a substantial regression in performance. I don't
   know why. It took more memory to allocate less? Each node had an extra byte to
   store the index, but that doesn't account for it.
** [2023-09-29 Fri] Transients

   first crack at transients and they do a lot more harm than good:

   2:   0.402445 seconds (670.46 k allocations: 44.595 MiB, 3.47% gc time, 99.94% compilation time)
   4:   0.000037 seconds (136 allocations: 3.953 KiB)
   8:   0.000051 seconds (260 allocations: 7.531 KiB)
   16:   0.137807 seconds (286.42 k allocations: 19.423 MiB, 99.81% compilation time)
   32:   0.127717 seconds (279.40 k allocations: 18.837 MiB, 99.75% compilation time)
   64:   0.000405 seconds (7.09 k allocations: 170.391 KiB)
   128:   0.001063 seconds (24.48 k allocations: 535.469 KiB)
   256:   0.003657 seconds (89.97 k allocations: 1.798 MiB)
   512:   0.013832 seconds (343.82 k allocations: 6.599 MiB)
   1024:   0.053068 seconds (1.34 M allocations: 25.202 MiB)
   2048:   0.241664 seconds (5.31 M allocations: 98.406 MiB, 13.25% gc time)
   4096:   0.915464 seconds (21.10 M allocations: 388.814 MiB, 4.25% gc time)
   8192:   3.544060 seconds (84.15 M allocations: 1.509 GiB, 3.20% gc time)
   16384:  14.464104 seconds (336.06 M allocations: 6.019 GiB, 3.89% gc time)
   abort...

   Locking is the first culprit to look into. Is there a way to enforce thread
   isolation at a higher level and not worry about locks?
** [2023-10-01 Sun] Transients Take Two
   2:   0.235361 seconds (433.40 k allocations: 28.939 MiB, 6.07% gc time, 99.92% compilation time)
   4:   0.000030 seconds (203 allocations: 5.906 KiB)
   8:   0.000031 seconds (387 allocations: 11.234 KiB)
   16:   0.189812 seconds (401.51 k allocations: 26.632 MiB, 99.90% compilation time)
   32:   0.115477 seconds (167.73 k allocations: 11.353 MiB, 12.64% gc time, 99.81% compilation time)
   64:   0.000239 seconds (3.54 k allocations: 105.094 KiB)
   128:   0.000391 seconds (7.17 k allocations: 213.625 KiB)
   256:   0.000780 seconds (14.44 k allocations: 430.000 KiB)
   512:   0.067743 seconds (205.24 k allocations: 12.381 MiB, 96.26% compilation time)
   1024:   0.007544 seconds (93.54 k allocations: 2.309 MiB)
   2048:   0.017824 seconds (208.90 k allocations: 5.000 MiB)
   4096:   0.038943 seconds (439.63 k allocations: 10.382 MiB)
   8192:   0.081237 seconds (901.09 k allocations: 21.144 MiB)
   16384:   1.162575 seconds (11.70 M allocations: 193.909 MiB, 4.27% gc time)
   32768:   4.961132 seconds (53.15 M allocations: 843.174 MiB, 1.58% gc time)
   65536:  12.987711 seconds (136.04 M allocations: 2.092 GiB, 1.90% gc time)
   fail...

   10x better than transients take I, but still 100x worse than straight up
   persistence...

   What the hell? I've got a lot to learn it would appear.

   removing all locking and checking makes a marginal difference:

   4:   0.000017 seconds (201 allocations: 5.859 KiB)
   8:   0.000016 seconds (385 allocations: 11.188 KiB)
   16:   0.000054 seconds (810 allocations: 23.781 KiB)
   32:   0.000091 seconds (1.72 k allocations: 50.953 KiB)
   64:   0.000173 seconds (3.54 k allocations: 105.047 KiB)
   128:   0.000350 seconds (7.17 k allocations: 213.578 KiB)
   256:   0.000709 seconds (14.43 k allocations: 429.953 KiB)
   512:   0.002295 seconds (35.85 k allocations: 986.891 KiB)
   1024:   0.007289 seconds (93.53 k allocations: 2.309 MiB)
   2048:   0.017355 seconds (208.90 k allocations: 5.000 MiB)
   4096:   0.057255 seconds (441.65 k allocations: 10.515 MiB, 20.47% gc time, 13.93% compilation time)
   8192:   0.076030 seconds (901.08 k allocations: 21.144 MiB)
   16384:   1.064113 seconds (11.70 M allocations: 193.909 MiB, 3.54% gc time)
   32768:   4.895505 seconds (53.15 M allocations: 843.174 MiB, 2.19% gc time)

   Curiourly, transients are faster for vectors of depth < 2 and use quite a bit
   less memory until 8k 2^13 nodes, but after that it reverses violently. Something
   strange is happening as the tree grows. I'm computing the count recursively
   instead of storing it. I'm an idiot.

   After storing the count in a field as per the persistent version:

   2:   0.000038 seconds (110 allocations: 3.188 KiB)
   4:   0.000014 seconds (203 allocations: 5.906 KiB)
   8:   0.000020 seconds (387 allocations: 11.234 KiB)
   16:   0.000069 seconds (780 allocations: 22.828 KiB)
   32:   0.000104 seconds (1.57 k allocations: 46.125 KiB)
   64:   0.000197 seconds (3.13 k allocations: 92.469 KiB)
   128:   0.000396 seconds (6.27 k allocations: 185.500 KiB)
   256:   0.000781 seconds (12.54 k allocations: 370.891 KiB)
   512:   0.001643 seconds (27.16 k allocations: 774.953 KiB)
   1024:   0.003485 seconds (58.45 k allocations: 1.577 MiB)
   2048:   0.007230 seconds (121.01 k allocations: 3.218 MiB)
   4096:   0.014666 seconds (246.15 k allocations: 6.500 MiB)
   8192:   0.043788 seconds (496.42 k allocations: 13.064 MiB, 30.56% gc time)
   16384:   0.061483 seconds (1.09 M allocations: 27.645 MiB)
   32768:   0.147103 seconds (2.48 M allocations: 59.745 MiB, 9.11% gc time)
   65536:   0.304465 seconds (5.24 M allocations: 123.945 MiB, 9.01% gc time)
   131072:   0.624119 seconds (10.78 M allocations: 252.346 MiB, 9.83% gc time)
   262144:   1.273259 seconds (21.85 M allocations: 509.146 MiB, 10.89% gc time)
   524288:   2.741977 seconds (47.05 M allocations: 1.044 GiB, 12.38% gc time)
   1048576:   5.857330 seconds (103.72 M allocations: 2.232 GiB, 12.94% gc time)

   We're catching up on time, and we're using *less* ram than Base.Vector. Though
   the difference is negligeble.

   This won't work well for multithreading though. I'll have to compare
   multithreaded persistent vectors with whever I can accomplish here.


   At this point, let's go back and check the comparison:


   #+BEGIN_SRC julia
     for i in 1:k
       print(string(2^i)*": ")
       @time rgb::Vector{UInt8} = reduce(vcat,
                                         map(p -> [
                                           reinterpret(UInt8, p.b), reinterpret(UInt8, p.g), reinterpret(UInt8, p.r),
                                           0xff
                                         ],
                                             image[1:2^i]
                                             )
                                         )
     end
   #+END_SRC

   and we get:

   2:   0.000011 seconds (8 allocations: 448 bytes)
   4:   0.000002 seconds (10 allocations: 624 bytes)
   8:   0.000002 seconds (14 allocations: 976 bytes)
   16:   0.000002 seconds (22 allocations: 1.625 KiB)
   32:   0.000002 seconds (38 allocations: 3.000 KiB)
   64:   0.000004 seconds (70 allocations: 5.734 KiB)
   128:   0.000004 seconds (134 allocations: 11.172 KiB)
   256:   0.000012 seconds (262 allocations: 22.156 KiB)
   512:   0.000015 seconds (518 allocations: 44.078 KiB)
   1024:   0.000028 seconds (1.03 k allocations: 87.609 KiB)
   2048:   0.000054 seconds (2.05 k allocations: 174.609 KiB)
   4096:   0.000102 seconds (4.11 k allocations: 348.375 KiB)
   8192:   0.000203 seconds (8.20 k allocations: 696.297 KiB)
   16384:   0.000520 seconds (16.39 k allocations: 1.360 MiB)
   32768:   0.000909 seconds (32.78 k allocations: 2.719 MiB)
   65536:   0.001815 seconds (65.55 k allocations: 5.438 MiB)
   131072:   0.003632 seconds (131.08 k allocations: 10.875 MiB)
   262144:   0.024924 seconds (262.15 k allocations: 21.750 MiB, 57.98% gc time)
   524288:   0.015855 seconds (524.30 k allocations: 43.500 MiB)
   1048576:   0.066121 seconds (1.05 M allocations: 87.000 MiB, 33.43% gc time)

   I'm so far off it's embarrassing. I must have included compile time without
   checking it before.

   So I'm still 2 orders of magnitude off on time and one on space...

   Special methods for creating small vectors make a noticable improvement:

   2:   0.000019 seconds (29 allocations: 944 bytes)
   4:   0.000005 seconds (40 allocations: 1.359 KiB)
   8:   0.000006 seconds (60 allocations: 2.125 KiB)
   16:   0.000034 seconds (125 allocations: 4.594 KiB)
   32:   0.000043 seconds (256 allocations: 9.641 KiB)
   64:   0.000078 seconds (512 allocations: 19.484 KiB)
   128:   0.000168 seconds (1.02 k allocations: 39.516 KiB)
   256:   0.000831 seconds (2.05 k allocations: 78.906 KiB)
   512:   0.000700 seconds (6.17 k allocations: 190.969 KiB)
   1024:   0.001800 seconds (16.46 k allocations: 447.203 KiB)
   2048:   0.003910 seconds (37.05 k allocations: 959.453 KiB)
   4096:   0.008681 seconds (78.22 k allocations: 1.938 MiB)
   8192:   0.015827 seconds (160.55 k allocations: 3.939 MiB)
   16384:   0.036072 seconds (420.48 k allocations: 9.395 MiB)
   32768:   0.075162 seconds (1.13 M allocations: 23.245 MiB)
   65536:   0.157719 seconds (2.56 M allocations: 50.945 MiB)
   131072:   0.339623 seconds (5.41 M allocations: 106.346 MiB)
   262144:   0.701219 seconds (11.11 M allocations: 217.146 MiB, 8.55% gc time)
   524288:   1.494626 seconds (25.55 M allocations: 485.249 MiB, 7.36% gc time)
   1048576:   3.373203 seconds (60.73 M allocations: 1.091 GiB, 10.46% gc time)

   Halving the ram usage. Not bad.

** [2023-10-01 Sun] More focused metrics

   These notes are a mess. But then so are my thoughts at the moment.

   Let's try another tack.

   Naive =into=
   #+BEGIN_SRC julia
     @time into(emptyvector, 1:2^20)
     1.649648 seconds (30.95 M allocations: 3.548 GiB, 26.17% gc time)
   #+END_SRC

   Transient =into=
   #+BEGIN_SRC julia
     @time into(emptyvector, 1:2^20)
     0.715259 seconds (17.95 M allocations: 318.826 MiB, 15.18% gc time)
   #+END_SRC

   alloc once recursive partitioning (current =vec= implementation).
   #+BEGIN_SRC julia
     @time vec(1:2^20)
     0.148071 seconds (3.40 M allocations: 98.187 MiB, 32.71% gc time)
   #+END_SRC

   The moral being that while transients help â€” a lot! â€” being clever helps even
   more.

   Of course, compared to native methods we're still out in the woods:

   #+BEGIN_SRC julia
     @time [i for i in 1:2^20]; nothing
     0.001074 seconds (2 allocations: 8.000 MiB)
   #+END_SRC

   So we're 100x slower and 10x heavier on ram than julia's datastructures. I'm
   pretty sure further optimisation of the transients is beyond me without moving
   to a lower level.

   Curiously, preallocating the accumulator in =leafpartition= and using an index
   (ring buffer) uses *more* ram (1.5x) but reduces runtime by a third.

   #+BEGIN_SRC julia
     function leafpartition(T)
       acc = Base.Vector{T}(undef, nodelength)
       i = 0
       function (emit)
         function inner()
           emit()
         end
         function inner(result)
           if i > 0
             emit(emit(result, [acc[j] for j in 1:i]))
           else
             emit(result)
           end
         end
         function inner(result, next)
           i+= 1
           acc[i] = next
           if i == nodelength
             t = copy(acc)
             i = 0
             emit(result, t)
           else
             result
           end
         end
         return inner
       end
     end

     @time  vec(1:2^20); nothing
     0.093256 seconds (3.37 M allocations: 144.004 MiB, 35.41% gc time)
   #+END_SRC

   Given the way vectors normally auto resize 8->40->..., I would have thought
   that we'd be saving ram this way. Odd. But the speedup is probably because
   there are more known types.

   Nope. Removing the preallocated buffer, but keeping the type argument makes
   everything worse...

   I'm running the measures again and it looks like the ring buffer method uses
   <10% more ram and runs 4x faster.

   I need a better test setup than @time and @profile...
** [2023-10-02 Mon] Times to beat

  #+BEGIN_SRC julia
    @time vec(1:2^20); nothing
    0.073854 seconds (3.37 M allocations: 144.005 MiB, 40.62% gc time)

    @time into(emptyvector, 1:2^20); nothing
    0.359100 seconds (11.55 M allocations: 207.188 MiB, 11.02% gc time)
  #+END_SRC
** Parallel transduction
   Stateless transducers are trivially parallel. Many stateful transducers can
   also be made parallel, but this is trickier. For now we can just rely on
   programmer annotations to tell us when a transduction is associative.

   A pipeline is associative if all components are.

   Reduction is harder because concat is a performance killer in the current
   persistent vector implementation.

   RRB Tries could solve this problem. I haven't read that paper in a long time
   though.

   Since merge on maps is associative and commutative, we ought to be able to
   parallelise mapish operations without too much ado.

   Now do I really want to try and implement something along the lines of Cilk?
** history of a waste of time
   I just removed the depth parameter from VectorNodes to see if the space
   savings (N/31 bytes for N elements in large vectors) was worth it. Why?
   Because I got carried away. Turns out I carried the depth around because I
   needed it to avoid doing a bunch of logarithms in nth and conj.

   Times for creation/nth/conj before "improvement"

   0.178549 seconds (8.79 M allocations: 503.427 MiB, 12.87% gc time)
   0.000755 seconds (22.00 k allocations: 453.109 KiB)
   0.000498 seconds (16.49 k allocations: 867.016 KiB)

   And times after:

   0.195730 seconds (8.79 M allocations: 503.385 MiB, 18.36% gc time)
   0.001861 seconds (32.44 k allocations: 930.531 KiB)
   0.000568 seconds (21.49 k allocations: 851.391 KiB)

   It actually needs more allocations to save a marginal amount of space.

   I just reverted all changes locally. I should have kept them in git for my own
   future reference. Should I have?
** Overriding =into= for =EmptyVector=
   I feel like an idiot for not thinking of this before. (into [] xs) is the same
   as vec(xs), so why not dispatch that way?

   standard benchmark:
   2:   0.000073 seconds (80 allocations: 2.625 KiB)
   4:   0.000020 seconds (126 allocations: 4.062 KiB)
   8:   0.000017 seconds (216 allocations: 6.875 KiB)
   16:   0.000031 seconds (570 allocations: 15.250 KiB)
   32:   0.000038 seconds (1.28 k allocations: 32.016 KiB)
   64:   0.000119 seconds (1.70 k allocations: 80.125 KiB)
   128:   0.000135 seconds (3.34 k allocations: 156.766 KiB)
   256:   0.000242 seconds (6.59 k allocations: 311.406 KiB)
   512:   0.000492 seconds (16.99 k allocations: 718.578 KiB)
   1024:   0.000990 seconds (37.79 k allocations: 1.497 MiB)
   2048:   0.002113 seconds (52.35 k allocations: 2.670 MiB)
   4096:   0.004327 seconds (104.62 k allocations: 5.345 MiB)
   8192:   0.008454 seconds (209.11 k allocations: 10.704 MiB)
   16384:   0.022399 seconds (542.04 k allocations: 25.210 MiB, 21.70% gc time)
   32768:   0.037470 seconds (1.21 M allocations: 54.223 MiB, 11.95% gc time)
   65536:   0.085009 seconds (1.75 M allocations: 121.065 MiB, 14.83% gc time)
   131072:   0.169703 seconds (3.49 M allocations: 242.178 MiB, 12.29% gc time)
   262144:   0.348428 seconds (6.98 M allocations: 484.417 MiB, 13.00% gc time)
   524288:   0.690695 seconds (18.75 M allocations: 1.087 GiB, 13.53% gc time)
   1048576:   1.360049 seconds (42.30 M allocations: 2.314 GiB, 13.18% gc time)

   vs transients:

   2:   0.000038 seconds (81 allocations: 2.375 KiB)
   4:   0.000015 seconds (142 allocations: 4.141 KiB)
   8:   0.000011 seconds (262 allocations: 7.547 KiB)
   16:   0.000055 seconds (525 allocations: 15.328 KiB)
   32:   0.000056 seconds (1.02 k allocations: 29.344 KiB)
   64:   0.000085 seconds (2.00 k allocations: 57.938 KiB)
   128:   0.000158 seconds (3.96 k allocations: 114.047 KiB)
   256:   0.000315 seconds (8.39 k allocations: 235.531 KiB)
   512:   0.000735 seconds (19.34 k allocations: 512.375 KiB)
   1024:   0.001510 seconds (41.21 k allocations: 1.040 MiB)
   2048:   0.003107 seconds (84.94 k allocations: 2.103 MiB)
   4096:   0.006271 seconds (172.41 k allocations: 4.255 MiB)
   8192:   0.012618 seconds (347.33 k allocations: 8.566 MiB)
   16384:   0.036978 seconds (794.50 k allocations: 18.686 MiB, 22.71% gc time)
   32768:   0.066359 seconds (1.69 M allocations: 38.665 MiB, 11.21% gc time)
   65536:   0.126810 seconds (3.48 M allocations: 78.894 MiB, 5.65% gc time)
   131072:   0.273211 seconds (7.05 M allocations: 159.365 MiB, 10.78% gc time)
   262144:   0.545332 seconds (14.21 M allocations: 320.323 MiB, 10.57% gc time)
   524288:   1.211752 seconds (31.66 M allocations: 689.117 MiB, 11.48% gc time)
   1048576:   2.509304 seconds (66.57 M allocations: 1.393 GiB, 12.58% gc time)

   These are both so much beter than the naive persistent reduction I won't even
   add it in.

   So the tranduction builder is about twice as fast and makes fewer allocations,
   but uses more scratch space. But the speedup is after the additional GC
   overhead.

   Ahh, there's an insidious bug in the above: the transform pipeline that builds
   the vector is based on the length of the input, but the prior pipeline is 4:1,
   thus the builder pipeline can only accomodate 1/4 of the data.

   Fixing this by hardcoding the correct number of layers gives us:

   2:   0.000152 seconds (177 allocations: 11.484 KiB)
   4:   0.000067 seconds (221 allocations: 15.281 KiB)
   8:   0.000065 seconds (304 allocations: 22.766 KiB)
   16:   0.000084 seconds (498 allocations: 39.703 KiB)
   32:   0.000097 seconds (860 allocations: 70.516 KiB)
   64:   0.000119 seconds (1.58 k allocations: 132.859 KiB)
   128:   0.000176 seconds (3.03 k allocations: 256.125 KiB)
   256:   0.000284 seconds (5.89 k allocations: 503.984 KiB)
   512:   0.000522 seconds (11.67 k allocations: 1002.172 KiB)
   1024:   0.000964 seconds (23.20 k allocations: 1.950 MiB)
   2048:   0.001869 seconds (46.26 k allocations: 3.874 MiB)
   4096:   0.007182 seconds (92.38 k allocations: 7.749 MiB, 48.77% gc time)
   8192:   0.007224 seconds (184.59 k allocations: 15.507 MiB)
   16384:   0.018153 seconds (369.05 k allocations: 31.036 MiB, 20.55% gc time)
   32768:   0.032288 seconds (737.96 k allocations: 61.833 MiB, 11.53% gc time)
   65536:   0.067171 seconds (1.48 M allocations: 123.697 MiB, 15.47% gc time)
   131072:   0.133401 seconds (2.95 M allocations: 247.439 MiB, 14.19% gc time)
   262144:   0.262998 seconds (5.90 M allocations: 494.937 MiB, 13.02% gc time)
   524288:   0.525131 seconds (11.81 M allocations: 988.827 M
   1048576:   1.052119 seconds (23.61 M allocations: 1.930 GiB, 13.67% gc time)

   which is a considerable improvement.

   Unfortunately, I'm at something of a loss as to how to dynamically create the
   correct pipeline. The problem with transducers is that once they start,
   they're locked because the reducer is passed in and so you can't add more
   steps at the end without unapplying that reducing function.

   Maybe I can create a clever reducing function...

   ...And a clever reducing function is the secret. *But* you have to make sure
   all state in stateful transducers is stored in the first inner closure, that
   way you can repeatedly apply the reducer without clearing the state.

   New times:

   2:   0.000065 seconds (100 allocations: 3.656 KiB)
   4:   0.000015 seconds (150 allocations: 5.453 KiB)
   8:   0.000018 seconds (246 allocations: 8.969 KiB)
   16:   0.000021 seconds (453 allocations: 16.469 KiB)
   32:   0.000036 seconds (867 allocations: 31.188 KiB)
   64:   0.000062 seconds (1.70 k allocations: 61.344 KiB)
   128:   0.000120 seconds (3.35 k allocations: 120.250 KiB)
   256:   0.000244 seconds (6.65 k allocations: 240.906 KiB)
   512:   0.000463 seconds (13.31 k allocations: 483.188 KiB)
   1024:   0.000912 seconds (26.63 k allocations: 968.234 KiB)
   2048:   0.001819 seconds (53.28 k allocations: 1.876 MiB)
   4096:   0.003708 seconds (106.57 k allocations: 3.762 MiB)
   8192:   0.007357 seconds (213.13 k allocations: 7.544 MiB)
   16384:   0.018811 seconds (428.30 k allocations: 15.259 MiB, 21.92% gc time)
   32768:   0.029789 seconds (858.64 k allocations: 30.430 MiB)
   65536:   0.067130 seconds (1.72 M allocations: 61.042 MiB, 11.79% gc time)
   131072:   0.129509 seconds (3.44 M allocations: 122.280 MiB, 8.24% gc time)
   262144:   0.256636 seconds (6.88 M allocations: 244.771 MiB, 7.31% gc time)
   524288:   0.527511 seconds (13.83 M allocations: 494.662 MiB, 8.60% gc time)
   1048576:   1.049643 seconds (27.74 M allocations: 994.445 MiB, 8.07% gc time)

   which is now faster than transients and use less ram. Given there's also a bug
   in transients that leads to a use after free (presumably something isn't being
   copied properly), I'm tempted to just cut them out entirely. They have some
   uses though.

   The times above involve calling transduce with a fixed xform and a funky
   reducer, but why not just call reduce? Effect:

   2:   0.000062 seconds (87 allocations: 2.766 KiB)
   4:   0.000015 seconds (145 allocations: 4.547 KiB)
   8:   0.000036 seconds (275 allocations: 8.938 KiB)
   16:   0.000046 seconds (530 allocations: 20.469 KiB)
   32:   0.000067 seconds (1.04 k allocations: 43.125 KiB)
   64:   0.000126 seconds (2.05 k allocations: 89.156 KiB)
   128:   0.000243 seconds (4.07 k allocations: 179.797 KiB)
   256:   0.000497 seconds (8.10 k allocations: 364.062 KiB)
   512:   0.001089 seconds (16.69 k allocations: 839.438 KiB)
   1024:   0.002231 seconds (33.85 k allocations: 1.749 MiB)
   2048:   0.004557 seconds (68.18 k allocations: 3.589 MiB)
   4096:   0.009160 seconds (136.84 k allocations: 7.296 MiB)
   8192:   0.019003 seconds (274.13 k allocations: 14.720 MiB)
   16384:   0.042296 seconds (581.53 k allocations: 36.219 MiB, 11.67% gc time)
   32768:   0.086952 seconds (1.20 M allocations: 78.958 MiB, 10.31% gc time)
   65536:   0.174166 seconds (2.43 M allocations: 164.707 MiB, 8.94% gc time)
   131072:   0.343993 seconds (4.89 M allocations: 336.218 MiB, 7.48% gc time)
   262144:   0.695022 seconds (9.80 M allocations: 679.257 MiB, 8.00% gc time)
   524288:   1.589509 seconds (20.16 M allocations: 1.470 GiB, 9.17% gc time)
   1048576:   3.353290 seconds (40.89 M allocations: 3.083 GiB, 10.07% gc time)

   What the hell?

   Hypothesis: since each step gets called 32x less than the step before it, the
   gains of caching xf(r) are tiny at the end, but huge in the beginning. So use
   a fixed pipeline as much as possible and go dynamic only at need.

   Just caching =tailxform(lastarg)= instead of apply it at every step gets us:
   2:   0.000077 seconds (98 allocations: 3.781 KiB)
   4:   0.000015 seconds (144 allocations: 5.578 KiB)
   8:   0.000017 seconds (230 allocations: 9.031 KiB)
   16:   0.000022 seconds (422 allocations: 16.578 KiB)
   32:   0.000036 seconds (804 allocations: 31.328 KiB)
   64:   0.000066 seconds (1.57 k allocations: 61.547 KiB)
   128:   0.000127 seconds (3.10 k allocations: 120.562 KiB)
   256:   0.000265 seconds (6.14 k allocations: 241.531 KiB)
   512:   0.000488 seconds (12.23 k allocations: 481.406 KiB)
   1024:   0.000967 seconds (24.40 k allocations: 961.516 KiB)
   2048:   0.001890 seconds (48.74 k allocations: 1.860 MiB)
   4096:   0.003788 seconds (97.43 k allocations: 3.727 MiB)
   8192:   0.007581 seconds (194.80 k allocations: 7.471 MiB)
   16384:   0.019740 seconds (389.52 k allocations: 14.968 MiB, 23.22% gc time)
   32768:   0.030088 seconds (778.97 k allocations: 29.703 MiB)
   65536:   0.070219 seconds (1.56 M allocations: 59.444 MiB, 13.97% gc time)
   131072:   0.129730 seconds (3.12 M allocations: 118.940 MiB, 7.22% gc time)
   262144:   0.269297 seconds (6.23 M allocations: 237.948 MiB, 10.41% gc time)
   524288:   0.531737 seconds (12.46 M allocations: 474.870 MiB, 9.54% gc time)
   1048576:   1.058747 seconds (24.92 M allocations: 948.718 MiB, 9.15% gc time)

   5% memory savings, essentially the same times. I need a more deterministic
   harness.
