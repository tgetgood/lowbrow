* Performance notes [2023-09-18 Mon]
  For normal use these datastructures seem to perform adequately.

  However, when trying to load textures with the following code:

#+BEGIN_SRC julia
  @time rgb = ds.into(
    ds.emptyvector,
    map(p -> [
        reinterpret(UInt8, p.b), reinterpret(UInt8, p.g), reinterpret(UInt8, p.r),
        0xff
      ])
    âˆ˜
    ds.cat(),
    ds.take(x, image)
  )
#+END_SRC

Where =image= is an array of pixels, I get some fun results. For x = 2-2^15:

2:   0.031788 seconds (46.67 k allocations: 9.054 MiB, 98.30% compilation time)
4:   0.006815 seconds (84 allocations: 12.007 MiB, 85.72% gc time)
8:   0.001803 seconds (164 allocations: 24.023 MiB)
16:   0.021166 seconds (3.82 k allocations: 48.276 MiB, 23.66% gc time, 40.25% compilation time)
32:   0.016689 seconds (866 allocations: 96.120 MiB, 41.37% gc time)
64:   0.065319 seconds (48.74 k allocations: 195.433 MiB, 15.55% gc time, 58.27% compilation time)
128:   0.044260 seconds (4.05 k allocations: 384.511 MiB, 38.96% gc time)
256:   0.075904 seconds (8.30 k allocations: 769.056 MiB, 31.04% gc time)
512:   0.143548 seconds (18.83 k allocations: 1.502 GiB, 32.47% gc time)
1024:   0.288988 seconds (39.91 k allocations: 3.005 GiB, 32.32% gc time)
2048:   0.570507 seconds (84.10 k allocations: 6.012 GiB, 32.30% gc time)
4096:   1.138775 seconds (172.48 k allocations: 12.030 GiB, 32.34% gc time)
8192:   2.278270 seconds (349.25 k allocations: 24.086 GiB, 32.43% gc time)
16384:   4.563693 seconds (702.79 k allocations: 48.270 GiB, 32.32% gc time)
32768:   9.203719 seconds (1.48 M allocations: 96.883 GiB, 32.35% gc time)

Which extrapolated up to the full 1M pixels of the image in question, this would
require 20M allocations and 3TB of allocated memory. Of course my machine runs
out of ram long before that can be tested.

The naive jl code:

#+BEGIN_SRC julia
  rgb::Vector{UInt8}  = reduce(vcat,
    map(p -> [
        reinterpret(UInt8, p.b), reinterpret(UInt8, p.g), reinterpret(UInt8, p.r),
        0xff
      ],
      image
    )
  )
#+END_SRC

0.577784 seconds (2.15 M allocations: 2.243 GiB, 25.86% gc time, 5.11% compilation

So the number of allocations in the persistent case is is ~10x higher and the
memory allocated and collected is >1000x higher.

10x on both counts would actually be acceptable for my exploratory use case, but
the 1000-1500x doesn't fly.

Curiously, if I use Base.Vector, it takes longer and uses more ram. It also seg
faults at 2^15 which I think is actually a stack overflow.

The idea was to use Base.Vector and Base.reduce and have better performace. That
needs a redesign.
* Performance improvements [2023-09-28 Thu]

After reimplementing vectors using Tuples instead of arrays (not to mention
fixing up some blatant errors, we get:

2:   0.037318 seconds (34.10 k allocations: 2.270 MiB)
4:   0.000030 seconds (147 allocations: 3.234 KiB)
8:   0.000036 seconds (299 allocations: 6.750 KiB)
16:   0.012085 seconds (4.31 k allocations: 261.349 KiB)
32:   0.021579 seconds (7.78 k allocations: 456.428 KiB)
64:   0.040643 seconds (14.73 k allocations: 852.820 KiB)
128:   0.079438 seconds (28.61 k allocations: 1.631 MiB)
256:   0.154938 seconds (56.38 k allocations: 3.331 MiB)
512:   0.005223 seconds (42.55 k allocations: 1.547 MiB)
1024:   0.014135 seconds (102.44 k allocations: 4.144 MiB)
2048:   0.049300 seconds (245.93 k allocations: 13.219 MiB, 25.63% gc time)
4096:   0.088340 seconds (565.65 k allocations: 33.909 MiB)
8192:   0.250066 seconds (1.35 M allocations: 76.654 MiB, 5.44% gc time)
16384:   0.580798 seconds (3.10 M allocations: 166.967 MiB, 4.01% gc time)
32768:   1.288246 seconds (6.79 M allocations: 368.141 MiB, 3.48% gc time)
65536:   2.873495 seconds (14.92 M allocations: 894.722 MiB, 3.85% gc time)
131072:   6.365209 seconds (32.24 M allocations: 1.982 GiB, 4.17% gc time)
262144:  14.536195 seconds (71.52 M allocations: 4.240 GiB, 3.77% gc time)
524288:  32.252954 seconds (155.95 M allocations: 8.910 GiB, 3.66% gc time)
1048576:  68.632605 seconds (330.91 M allocations: 18.891 GiB, 3.62% gc time)

~10x improvement in run time. Huge improvement in allocated space, but number of
allocations is ~6x higher.

Lesson 1) transients are indispensible. Lesson 2) maybe tuples weren't such a
hot idea after all.

I'll try out the streamlined design with vectors again. I might be thrashing
here.

Easy wins are parallel transduction and transients. Maybe I should focus my
efforts there.

I also massively simplified the implementation of maps, and added some tests to
convince myself they work (to the extent I've thought to use them). But again
huge performance regression. They're basically unusable as is.

After reimplementing with (base) vectors instead of tuples, I get:
2:   0.000036 seconds (68 allocations: 2.328 KiB)
4:   0.000011 seconds (138 allocations: 4.891 KiB)
8:   0.000017 seconds (278 allocations: 10.938 KiB)
16:   0.000050 seconds (930 allocations: 37.641 KiB)
32:   0.000094 seconds (2.35 k allocations: 95.234 KiB)
64:   0.000207 seconds (5.57 k allocations: 225.328 KiB)
128:   0.000493 seconds (13.54 k allocations: 619.172 KiB)
256:   0.001366 seconds (36.66 k allocations: 1.819 MiB)
512:   0.003244 seconds (88.02 k allocations: 4.183 MiB)
1024:   0.015228 seconds (193.80 k allocations: 9.068 MiB, 59.99% gc time)
2048:   0.012516 seconds (417.67 k allocations: 19.494 MiB)
4096:   0.036217 seconds (914.55 k allocations: 45.283 MiB, 24.31% gc time)
8192:   0.079682 seconds (2.10 M allocations: 113.657 MiB, 18.27% gc time)
16384:   0.158852 seconds (4.74 M allocations: 245.702 MiB, 13.64% gc time)
32768:   0.338404 seconds (10.12 M allocations: 514.794 MiB, 14.86% gc time)
65536:   0.701287 seconds (21.26 M allocations: 1.049 GiB, 14.67% gc time)
131072:   1.523102 seconds (45.11 M allocations: 2.295 GiB, 16.00% gc time)
262144:   3.396177 seconds (99.11 M allocations: 5.313 GiB, 16.90% gc time)
524288:   7.134940 seconds (215.47 M allocations: 11.204 GiB, 17.91% gc time)
1048576:  14.954999 seconds (451.32 M allocations: 23.141 GiB, 19.62% gc time)

With no runtime compiler warmup to speak of, which is an added bonus.

so now we're down to 10x ram, 30x runtime, and 200x allocations. But that's
without transients. Progress.

And to think I went with tuples because I thought they would optimise better
being immutable. And they do in terms of allocations and memory usage. But 1.28x
more RAM for a 4.5x runtime boost seems like a good trade.

[2023-09-29 Fri]

2:   0.000040 seconds (72 allocations: 2.141 KiB)
4:   0.000011 seconds (142 allocations: 4.453 KiB)
8:   0.000018 seconds (282 allocations: 10.000 KiB)
16:   0.000050 seconds (866 allocations: 32.734 KiB)
32:   0.000095 seconds (2.14 k allocations: 81.078 KiB)
64:   0.000218 seconds (5.07 k allocations: 189.672 KiB)
128:   0.000470 seconds (12.47 k allocations: 528.500 KiB)
256:   0.001161 seconds (33.42 k allocations: 1.561 MiB)
512:   0.002527 seconds (77.35 k allocations: 3.539 MiB)
1024:   0.005505 seconds (168.29 k allocations: 7.620 MiB)
2048:   0.011057 seconds (362.45 k allocations: 16.345 MiB)
4096:   0.032831 seconds (799.94 k allocations: 38.357 MiB, 28.70% gc time)
8192:   0.071598 seconds (1.87 M allocations: 97.677 MiB, 23.86% gc time)
16384:   0.130356 seconds (4.11 M allocations: 209.177 MiB, 12.99% gc time)
32768:   0.281846 seconds (8.69 M allocations: 436.178 MiB, 16.36% gc time)
65536:   0.582891 seconds (18.24 M allocations: 908.179 MiB, 16.01% gc time)
131072:   1.223732 seconds (38.90 M allocations: 1.951 GiB, 16.16% gc time)
262144:   2.918031 seconds (86.53 M allocations: 4.558 GiB, 17.62% gc time)
524288:   6.131520 seconds (184.93 M allocations: 9.550 GiB, 18.30% gc time)
1048576:  12.946524 seconds (384.87 M allocations: 19.658 GiB, 20.43% gc time)

Just by overloading conj with a fast case when the element being added is a
subtype of the collection type.

I didn't expect reflection (typejoin) to be cheap, but that one call can't
account for what's going on here. Looking at the LLVM bitcode, the generate code
is superficially similar, but I'm not familiar enough to compare them without
great effort which I'd rather allocate elsewhere.

Maybe allowing the possibility of having to box at every step causes all the
extra work. In any case I've learned to be much more wary of reflection.

And removing the asserts gets us:

2:   0.266746 seconds (696.97 k allocations: 46.322 MiB, 99.87% compilation time)
4:   0.000036 seconds (142 allocations: 4.453 KiB)
8:   0.000034 seconds (282 allocations: 10.000 KiB)
16:   0.188647 seconds (130.75 k allocations: 8.612 MiB, 8.30% gc time, 99.93% compilation time)
32:   0.000098 seconds (1.85 k allocations: 72.125 KiB)
64:   0.000166 seconds (3.95 k allocations: 161.656 KiB)
128:   0.000365 seconds (8.14 k allocations: 438.359 KiB)
256:   0.000868 seconds (16.52 k allocations: 1.250 MiB)
512:   0.008639 seconds (42.78 k allocations: 2.864 MiB, 77.68% compilation time)
1024:   0.003965 seconds (94.47 k allocations: 6.089 MiB)
2048:   0.008332 seconds (198.40 k allocations: 12.750 MiB)
4096:   0.033216 seconds (406.25 k allocations: 29.133 MiB, 47.31% gc time)
8192:   0.049455 seconds (821.97 k allocations: 70.945 MiB, 25.66% gc time)
16384:   0.100161 seconds (1.98 M allocations: 153.680 MiB, 16.17% gc time)
32768:   0.188255 seconds (4.29 M allocations: 320.151 MiB, 10.64% gc time)
65536:   0.394972 seconds (8.92 M allocations: 659.092 MiB, 12.61% gc time)
131072:   0.855680 seconds (18.18 M allocations: 1.401 GiB, 13.63% gc time)
262144:   1.972930 seconds (36.69 M allocations: 3.199 GiB, 15.42% gc time)
524288:   4.259396 seconds (84.20 M allocations: 6.769 GiB, 17.15% gc time)
1048576:   8.968445 seconds (179.22 M allocations: 13.939 GiB, 18.54% gc time)

Which really oughtn't be surprising...

One idea which I ought to look into is preallocating vectorleaves in certain
contexts. Creating vectors with Vector(undef, 32) and tracking indicies instead
of using `end` actually caused a substantial regression in performance. I don't
know why. It took more memory to allocate less? Each node had an extra byte to
store the index, but that doesn't account for it.
